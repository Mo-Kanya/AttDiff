{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FID TensorFlow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4dcd82b5eb5a4037a54dbb41c67967ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c56c6a853cec4e15ab08ad8fe511aa5a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d1499b6a6a774ead962aa6c713600746",
              "IPY_MODEL_9a04181950a04055a7ded9a5f5139942",
              "IPY_MODEL_8eac00d251ff4a19b611de2dcccf7ed9"
            ]
          }
        },
        "c56c6a853cec4e15ab08ad8fe511aa5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d1499b6a6a774ead962aa6c713600746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_98439b1b8fb74018b88a18c643053397",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2dfc2645a8d04ed39a05f83176aa8658"
          }
        },
        "9a04181950a04055a7ded9a5f5139942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e5c5d0393de544879c3c9368128a9162",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 95628359,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 95628359,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7612606090a1407b8dcf0eadd9628a10"
          }
        },
        "8eac00d251ff4a19b611de2dcccf7ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d9ebc5b7dda5415dbe2f4ccfcc1316fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 91.2M/91.2M [00:04&lt;00:00, 21.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15faee2ce7cb42019e230e557a7368b4"
          }
        },
        "98439b1b8fb74018b88a18c643053397": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2dfc2645a8d04ed39a05f83176aa8658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5c5d0393de544879c3c9368128a9162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7612606090a1407b8dcf0eadd9628a10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9ebc5b7dda5415dbe2f4ccfcc1316fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15faee2ce7cb42019e230e557a7368b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-fid\n",
        "from pytorch_fid.fid_score import calculate_fid_given_paths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUypM-W21mM9",
        "outputId": "1b9abf30-1e64-408d-9dec-ac2afdc413ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-fid\n",
            "  Downloading pytorch-fid-0.2.1.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-fid) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.1->pytorch-fid) (3.10.0.2)\n",
            "Building wheels for collected packages: pytorch-fid\n",
            "  Building wheel for pytorch-fid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fid: filename=pytorch_fid-0.2.1-py3-none-any.whl size=14835 sha256=9bc47cce67be8b20665a874aa1c762c24d53b61cb6a28f4f694fa789e7c987b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/ac/03/c5634775c8a64f702343ef5923278f8d3bb8c651debc4a6890\n",
            "Successfully built pytorch-fid\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0XuqNyMUPzB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from zipfile import ZipFile\n",
        "import requests\n",
        "import tqdm\n",
        "import zipfile\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "from io import BytesIO\n",
        "from PIL import ImageDraw\n",
        "from PIL import ImageFont\n",
        "from shutil import copyfile\n",
        "import six.moves.cPickle as cPickle\n",
        "from typing import Optional, Tuple, List\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.layers import Normalization\n",
        "from keras.preprocessing.image import save_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "\n",
        "import pandas as pd\n",
        "import natsort\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from random import randint\n",
        "from numpy import iscomplexobj\n",
        "from scipy.linalg import sqrtm\n",
        "from keras.applications.inception_v3 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = {'id': id, 'confirm': token}\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    save_response_content(response, destination)\n",
        "    unpack_and_remove(destination)\n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    chunk_size = 32768\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(chunk_size):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "    \n",
        "def unpack_and_remove(destination):\n",
        "    file_content = zipfile.ZipFile(destination)\n",
        "    file_content.extractall()\n",
        "    os.remove(destination)\n",
        "    \n",
        "file_id = '1GiktHkovd9Ii2nHNfE4P4Xkxu4GUkNRI'\n",
        "destination = '/content/tensorflow-inception_images.zip'\n",
        "download_file_from_google_drive(file_id, destination)"
      ],
      "metadata": {
        "id": "iIoP9LISUQoQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 14\n",
        "label_size = 2\n",
        "resolution = 256"
      ],
      "metadata": {
        "id": "ugxZJSNwpJ_7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = './celab_a_age/'\n",
        "data_path = path + 'examples_1.tfrecord'\n",
        "bucket_path = 'https://storage.googleapis.com/explaining-in-style/celeb_a_age/'\n",
        "original_images = \"./original_images\"\n",
        "data_folder = \"../input/flickrfaceshq-dataset-ffhq\""
      ],
      "metadata": {
        "id": "DKz5a67b0ToM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_path_from_bucket(bucket_url, path):\n",
        "    r = requests.get(bucket_url)\n",
        "    filename = os.path.split(bucket_url)[-1].replace('.zip', '')\n",
        "    zip_ref = zipfile.ZipFile(BytesIO(r.content))\n",
        "    zip_ref.extractall(path)\n",
        "    return os.path.join(path, filename)\n",
        "\n",
        "generator = tf.keras.models.load_model(get_path_from_bucket(bucket_path + 'generator.savedmodel.zip', path))\n",
        "encoder = tf.keras.models.load_model(get_path_from_bucket(bucket_path + 'encoder.savedmodel.zip', path))\n",
        "discriminator = tf.keras.models.load_model(get_path_from_bucket(bucket_path + 'discriminator.savedmodel.zip', path))\n",
        "classifier = tf.keras.models.load_model(get_path_from_bucket(bucket_path + 'mobilenet.savedmodel.zip', path))\n",
        "inception = tf.keras.applications.inception_v3.InceptionV3(include_top=False, input_shape=(299, 299, 3), pooling='avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtOcw3FC0Ttl",
        "outputId": "749467e1-9a38-43f6-bcd9-485d343a0866"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_unstable_images(style_change_effect: np.ndarray,\n",
        "                           effect_threshold: float = 0.3,\n",
        "                           num_indices_threshold: int = 750):\n",
        "\n",
        "    unstable_images = (np.sum(np.abs(style_change_effect) > effect_threshold, axis=(1, 2, 3)) > num_indices_threshold)\n",
        "    style_change_effect[unstable_images] = 0\n",
        "    return style_change_effect"
      ],
      "metadata": {
        "id": "iLQUf9nI0Tv3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples_url = bucket_path + 'examples_1.tfrecord'\n",
        "r = requests.get(examples_url)\n",
        "data_path = path + 'examples_1.tfrecord'\n",
        "open(data_path, 'wb').write(r.content)\n",
        "num_classes = 2\n",
        "print(f'Loaded dataset: {data_path}')\n",
        "table = tf.data.TFRecordDataset([data_path])\n",
        "# Read sspace tfrecord unwrapped:\n",
        "style_change_effect = []\n",
        "latents = []\n",
        "base_probs = []\n",
        "\n",
        "for raw_record in table:\n",
        "    example = tf.train.Example()\n",
        "    example.ParseFromString(raw_record.numpy())\n",
        "    latents.append(np.array(example.features.feature['dlatent'].float_list.value))\n",
        "    seffect = np.array(example.features.feature['result'].float_list.value).reshape((-1, 2, num_classes))\n",
        "    style_change_effect.append(seffect.transpose([1, 0, 2]))\n",
        "    base_probs.append(np.array(example.features.feature['base_prob'].float_list.value))\n",
        "\n",
        "style_change_effect = np.array(style_change_effect)\n",
        "latents = np.array(latents)\n",
        "W_values, style_change_effect, base_probs = latents, style_change_effect, np.array(base_probs)\n",
        "\n",
        "style_change_effect = filter_unstable_images(style_change_effect, effect_threshold=2)\n",
        "\n",
        "all_style_vectors = tf.concat(generator.style_vector_calculator(W_values, training=False)[0], axis=1).numpy()\n",
        "style_min = np.min(all_style_vectors, axis=0)\n",
        "style_max = np.max(all_style_vectors, axis=0)\n",
        "\n",
        "all_style_vectors_distances = np.zeros((all_style_vectors.shape[0], all_style_vectors.shape[1], 2))\n",
        "all_style_vectors_distances[:,:, 0] = all_style_vectors - np.tile(style_min, (all_style_vectors.shape[0], 1))\n",
        "all_style_vectors_distances[:,:, 1] = np.tile(style_max, (all_style_vectors.shape[0], 1)) - all_style_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zToTbq_h0Txl",
        "outputId": "35998dc0-9d8c-4604-d2df-65bf570144b4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset: ./celab_a_age/examples_1.tfrecord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_image(image, fmt='png'):\n",
        "    if image.dtype == np.float32:\n",
        "        image = np.uint8(image * 127.5 + 127.5)\n",
        "    if image.shape[0] == 3:\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "    bytes_io = BytesIO()\n",
        "    Image.fromarray(image).save(bytes_io, fmt)\n",
        "    IPython.display.display(IPython.display.Image(data=bytes_io.getvalue()))"
      ],
      "metadata": {
        "id": "sSdsPzCk0bs7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = np.argmax(base_probs, axis=1)\n",
        "style_effect_classes = {}\n",
        "W_classes = {}\n",
        "style_vectors_distances_classes = {}\n",
        "all_style_vectors_classes = {}\n",
        "for img_ind in range(label_size):\n",
        "    img_inx = np.array([i for i in range(all_labels.shape[0]) if all_labels[i] == img_ind])\n",
        "    curr_style_effect = np.zeros((len(img_inx), style_change_effect.shape[1], style_change_effect.shape[2], style_change_effect.shape[3]))\n",
        "    curr_w = np.zeros((len(img_inx), W_values.shape[1]))\n",
        "    curr_style_vector_distances = np.zeros((len(img_inx), style_change_effect.shape[2], 2))\n",
        "    for k, i in enumerate(img_inx):\n",
        "        curr_style_effect[k, :, :] = style_change_effect[i, :, :, :]\n",
        "        curr_w[k, :] = W_values[i, :]\n",
        "        curr_style_vector_distances[k, :, :] = all_style_vectors_distances[i, :, :]\n",
        "    style_effect_classes[img_ind] = curr_style_effect\n",
        "    W_classes[img_ind] = curr_w\n",
        "    style_vectors_distances_classes[img_ind] = curr_style_vector_distances\n",
        "    all_style_vectors_classes[img_ind] = all_style_vectors[img_inx]\n",
        "    print(f'Class {img_ind}, {len(img_inx)} images.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAZXS6mM0fp5",
        "outputId": "eae8bfc4-8c44-4cc6-8c94-60f47bc0e951"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0, 90 images.\n",
            "Class 1, 160 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator_results_given_dlatent(dlatent: np.ndarray,\n",
        "                                            generator: tf.keras.models.Model,\n",
        "                                            discriminator: tf.keras.models.Model,\n",
        "                                            classifier: tf.keras.models.Model,\n",
        "                                            class_index: int,\n",
        "                                            sindex: int,\n",
        "                                            s_style_min: float,\n",
        "                                            s_style_max: float,\n",
        "                                            style_direction_index: int,\n",
        "                                            shift_size: float = 2,\n",
        "                                            label_size: int = 2):\n",
        "\n",
        "    network_inputs = generator.style_vector_calculator(dlatent)\n",
        "    images_out = generator.g_synthesis(network_inputs, training=False)\n",
        "    images_out = tf.maximum(tf.minimum(images_out, 1), -1)\n",
        "    labels = tf.constant(dlatent[:, -label_size:], dtype=tf.float32)\n",
        "    \n",
        "    discriminator_before = discriminator([images_out, labels], training=False)\n",
        "    print(\"output discriminator\", discriminator_before)\n",
        "    print(\"output discriminator shape\", discriminator_before.shape)\n",
        "  \n",
        "    # I am not using the classifier output here, because it is only one.\n",
        "    change_image, _ = (generate_change_image_given_dlatent(dlatent, generator, classifier,\n",
        "                                                              class_index, sindex,\n",
        "                                                              s_style_min, s_style_max,\n",
        "                                                              style_direction_index, shift_size,\n",
        "                                                              label_size))\n",
        "    \n",
        "    \n",
        "    labels = tf.nn.softmax(classifier(change_image, training=False))\n",
        "    change_image_for_disc = tf.transpose(change_image, (0, 3, 1, 2))\n",
        "    discriminator_after = discriminator([change_image_for_disc, labels], training=False)\n",
        "    \n",
        "    return (discriminator_before, discriminator_after)"
      ],
      "metadata": {
        "id": "wJPX0ctN0fsv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_filter(style_change_effect: np.ndarray,\n",
        "                         all_dlatents: np.ndarray,\n",
        "                         generator: tf.keras.models.Model,\n",
        "                         discriminator: tf.keras.models.Model,\n",
        "                         classifier: tf.keras.models.Model,\n",
        "                         sindex: int,\n",
        "                         style_min: float,\n",
        "                         style_max: float,\n",
        "                         class_index: int,\n",
        "                         num_images: int = 10,\n",
        "                         label_size: int = 2,\n",
        "                         change_threshold: float = 0.5,\n",
        "                         shift_size: float = 2,\n",
        "                         effect_threshold: float = 0.2,\n",
        "                         sindex_offset: int = 0):\n",
        "\n",
        "    for style_sign_index in range(2):\n",
        "        images_idx = ((style_change_effect[:, style_sign_index, sindex, class_index]) > effect_threshold).nonzero()[0]\n",
        "\n",
        "        images_idx = images_idx[:num_images]\n",
        "        dlatents = all_dlatents[images_idx]\n",
        "\n",
        "        for i in range(len(images_idx)):\n",
        "            cur_dlatent = dlatents[i:i + 1]\n",
        "            (discriminator_orig, discriminator_change) = get_discriminator_results_given_dlatent(\n",
        "                                                               dlatent=cur_dlatent,\n",
        "                                                               generator=generator,\n",
        "                                                               discriminator=discriminator,\n",
        "                                                               classifier=classifier,\n",
        "                                                               class_index=class_index,\n",
        "                                                               sindex=sindex + sindex_offset,\n",
        "                                                               s_style_min=style_min,\n",
        "                                                               s_style_max=style_max,\n",
        "                                                               style_direction_index=style_sign_index,\n",
        "                                                               shift_size=shift_size,\n",
        "                                                               label_size=label_size)\n",
        "\n",
        "            if np.abs(discriminator_orig - discriminator_change) > change_threshold:\n",
        "                return False\n",
        "    return True"
      ],
      "metadata": {
        "id": "YHhnPL4t0fvH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_significant_styles(\n",
        "    style_change_effect: np.ndarray,\n",
        "    num_indices: int,\n",
        "    class_index: int,\n",
        "    discriminator: Optional[tf.keras.models.Model],\n",
        "    generator: tf.keras.models.Model,\n",
        "    classifier: tf.keras.models.Model,\n",
        "    all_dlatents: np.ndarray,\n",
        "    style_min: np.ndarray,\n",
        "    style_max: np.ndarray,\n",
        "    max_image_effect: float = 0.2,\n",
        "    label_size: int = 2,\n",
        "    discriminator_threshold: float = 0.2,\n",
        "    sindex_offset: int = 0):\n",
        "\n",
        "    num_images = style_change_effect.shape[0]\n",
        "    style_effect_direction = np.maximum(0, style_change_effect[:, :, :, class_index].reshape((num_images, -1)))\n",
        "\n",
        "    images_effect = np.zeros(num_images)\n",
        "    all_sindices = []\n",
        "    discriminator_removed = []\n",
        "    while len(all_sindices) < num_indices:\n",
        "        next_s = np.argmax(np.mean(style_effect_direction[images_effect < max_image_effect], axis=0))\n",
        "        if discriminator is not None:\n",
        "            sindex = next_s % style_change_effect.shape[2]\n",
        "            if sindex == 0:\n",
        "                break\n",
        "            if not discriminator_filter(\n",
        "                      style_change_effect=style_change_effect,\n",
        "                      all_dlatents=all_dlatents,\n",
        "                      generator=generator,\n",
        "                      discriminator=discriminator,\n",
        "                      classifier=classifier,\n",
        "                      sindex=sindex,\n",
        "                      style_min=style_min[sindex + sindex_offset],\n",
        "                      style_max=style_max[sindex + sindex_offset],\n",
        "                      class_index=class_index,\n",
        "                      label_size=label_size,\n",
        "                      change_threshold=discriminator_threshold,\n",
        "                      sindex_offset=sindex_offset):\n",
        "                style_effect_direction[:, next_s] = np.zeros(num_images)\n",
        "                discriminator_removed.append(sindex)\n",
        "                continue\n",
        "\n",
        "        all_sindices.append(next_s)\n",
        "        images_effect += style_effect_direction[:, next_s]\n",
        "        style_effect_direction[:, next_s] = 0\n",
        "\n",
        "    return [(x // style_change_effect.shape[2],(x % style_change_effect.shape[2]) + sindex_offset) for x in all_sindices]"
      ],
      "metadata": {
        "id": "ne_X-Ydw0fxo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_size_clasifier = 2\n",
        "num_indices =  10\n",
        "effect_threshold = 0.2\n",
        "use_discriminator = False\n",
        "discriminator_model = discriminator if use_discriminator else None\n",
        "s_indices_and_signs_dict = {}\n",
        "\n",
        "for class_index in [0, 1]:\n",
        "    split_ind = 1 - class_index\n",
        "    all_s = style_effect_classes[split_ind]\n",
        "    all_w = W_classes[split_ind]\n",
        "\n",
        "    # Find s indicies\n",
        "    s_indices_and_signs = find_significant_styles(\n",
        "        style_change_effect=all_s,\n",
        "        num_indices=num_indices,\n",
        "        class_index=class_index,\n",
        "        discriminator=discriminator_model,\n",
        "        generator=generator,\n",
        "        classifier=classifier,\n",
        "        all_dlatents=all_w,\n",
        "        style_min=style_min,\n",
        "        style_max=style_max,\n",
        "        max_image_effect=effect_threshold*5,\n",
        "        label_size=label_size_clasifier,\n",
        "        discriminator_threshold=0.2,\n",
        "        sindex_offset=0)\n",
        "\n",
        "    s_indices_and_signs_dict[class_index] = s_indices_and_signs\n",
        "\n",
        "\n",
        "sindex_class_0 = [sindex for _, sindex in s_indices_and_signs_dict[0]]\n",
        "\n",
        "all_sindex_joined_class_0 = [(1 - direction, sindex) for direction, sindex in \n",
        "                             s_indices_and_signs_dict[1] if sindex not in sindex_class_0]\n",
        "all_sindex_joined_class_0 += s_indices_and_signs_dict[0]\n",
        "\n",
        "scores = []\n",
        "\n",
        "for direction, sindex in all_sindex_joined_class_0:\n",
        "    other_direction = 1 if direction == 0 else 0\n",
        "    curr_score = np.mean(style_change_effect[:, direction, sindex, 0]) + np.mean(style_change_effect[:, other_direction, sindex, 1])\n",
        "    scores.append(curr_score)\n",
        "\n",
        "s_indices_and_signs = [all_sindex_joined_class_0[i] for i in np.argsort(scores)[::-1]]\n",
        "print('Directions and style indices for moving from class 1 to class 0 = ', s_indices_and_signs[:num_indices])\n",
        "print('Use the other direction to move for class 0 to 1.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSspaupm0fzg",
        "outputId": "59f08ba0-c14d-4807-f71d-3704afe48342"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directions and style indices for moving from class 1 to class 0 =  [(1, 5300), (0, 5147), (1, 3301), (1, 3199), (1, 3921), (1, 1208), (0, 4519), (0, 5246), (0, 4172), (1, 4469)]\n",
            "Use the other direction to move for class 0 to 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classifier_results(generator: tf.keras.models.Model, expanded_dlatent: tf.Tensor, use_softmax: bool = False):\n",
        "    image = call_synthesis(generator, expanded_dlatent)\n",
        "    image = tf.transpose(image, (0, 2, 3, 1))\n",
        "    results = classifier(image, training=False)\n",
        "    if use_softmax:\n",
        "        return tf.nn.softmax(results).numpy()[0]\n",
        "    else:\n",
        "        return results.numpy()[0]"
      ],
      "metadata": {
        "id": "vryE86Hv0mPU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_synthesis(generator: tf.keras.models.Model,\n",
        "                   dlatents_in: tf.Tensor,\n",
        "                   conditioning_in: Optional[tf.Tensor] = None,\n",
        "                   labels_in: Optional[tf.Tensor] = None,\n",
        "                   training: bool = False,\n",
        "                   num_layers: int = 14,\n",
        "                   dlatent_size: int = 512) -> tf.Tensor:\n",
        "\n",
        "    if labels_in is not None:\n",
        "        zero_labels = tf.zeros_like(labels_in)\n",
        "        dlatents_labels = tf.tile(tf.expand_dims(labels, 1), [1, num_layers, 1])\n",
        "        if dlatent_size > 0:\n",
        "            dlatents_expanded = tf.concat([dlatents_in, dlatents_labels], axis=2)\n",
        "        else:\n",
        "            dlatents_expanded = dlatents_labels\n",
        "    else:\n",
        "        if dlatent_size == 0:\n",
        "            raise ValueError('Dlatents are empty and no labels were provided.')\n",
        "        dlatents_expanded = dlatents_in\n",
        "\n",
        "    style_vector_blocks, style_vector_torgb = generator.style_vector_calculator(dlatents_expanded[:, 0], training=training)\n",
        "    \n",
        "    if conditioning_in is not None:\n",
        "        network_inputs = (style_vector_blocks, style_vector_torgb, conditioning_in)\n",
        "    else:\n",
        "        network_inputs = (style_vector_blocks, style_vector_torgb)\n",
        "        synthesis_results = generator.g_synthesis(network_inputs, training=training)\n",
        "\n",
        "    return tf.maximum(tf.minimum(synthesis_results, 1), -1)"
      ],
      "metadata": {
        "id": "CEe08ik30mR6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAYER_SHAPES = []\n",
        "\n",
        "for dense in generator.style_vector_calculator.style_dense_blocks:\n",
        "    LAYER_SHAPES.append(dense.dense_bias.weights[0].shape[1])\n",
        "\n",
        "def sindex_to_layer_idx_and_index(generator: tf.keras.models.Model,\n",
        "                                  sindex: int) -> Tuple[int, int]:\n",
        "    global LAYER_SHAPES\n",
        "    layer_shapes_cumsum = np.concatenate([[0], np.cumsum(LAYER_SHAPES)])\n",
        "    layer_idx = (layer_shapes_cumsum <= sindex).nonzero()[0][-1]\n",
        "    return layer_idx, sindex - layer_shapes_cumsum[layer_idx]"
      ],
      "metadata": {
        "id": "A0d2bTSj0mUc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_latents(latents, generator, num_images, batch_size, image_size=(256, 256), channels=3):\n",
        "    \n",
        "    h, w = image_size\n",
        "    \n",
        "    generated_images = []\n",
        "    \n",
        "    for latent in tqdm.tqdm(latents):\n",
        "        expanded_latent = tf.tile(tf.expand_dims(np.expand_dims(latent, 0), 1), [1, 14, 1])\n",
        "        generated_image = call_synthesis(generator, expanded_latent, num_layers=14)\n",
        "        generated_image = tf.transpose(generated_image, perm=[0, 2, 3, 1])\n",
        "        generated_image = generated_image.cpu()\n",
        "        generated_images.append(generated_image)\n",
        "\n",
        "    generated_dataset = tf.stack(generated_images)\n",
        "    generated_dataset = tf.reshape(generated_dataset, [num_images // batch_size, batch_size, h, w, channels])\n",
        "\n",
        "    return generated_dataset"
      ],
      "metadata": {
        "id": "ngmnqtuk0mWq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_decode(dataset, generator, encoder, classifier, normalize_layer):\n",
        "    \n",
        "    generated_batches = []\n",
        "\n",
        "    for batch in tqdm.tqdm(dataset):\n",
        "        \n",
        "        batch = normalize_layer(batch)\n",
        "        classification = tf.nn.softmax(classifier(batch, training=False))\n",
        "        batch = tf.transpose(batch, perm=[0, 3, 1, 2])\n",
        "\n",
        "        output_encoder = encoder.predict_on_batch(batch)\n",
        "        latent = tf.concat([output_encoder, classification], -1)\n",
        "        expanded_latent = tf.tile(tf.expand_dims(latent, 1), [1, 14, 1])\n",
        "\n",
        "        generated_batch = call_synthesis(generator, expanded_latent, num_layers=14)\n",
        "        generated_batch = tf.transpose(generated_batch, perm=[0, 2, 3, 1])\n",
        "        generated_batch = generated_batch.cpu()\n",
        "        generated_batches.append(generated_batch)\n",
        "        \n",
        "        del batch\n",
        "        del generated_batch\n",
        "    \n",
        "    generated_dataset = tf.stack(generated_batches).cpu()\n",
        "\n",
        "    return generated_dataset"
      ],
      "metadata": {
        "id": "C1pLY0en0mY9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inception(batch, size, generated, normalize):\n",
        "    \n",
        "    batch = tf.image.resize(batch, size)\n",
        "    if not generated:\n",
        "        batch = normalize(batch)\n",
        "        \n",
        "    return batch"
      ],
      "metadata": {
        "id": "5bXEsekl0mbP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_counterfactual_dataset(k,\n",
        "                                  latents,\n",
        "                                  generator,\n",
        "                                  classifier,\n",
        "                                  s_indices_and_signs,\n",
        "                                  style_min,\n",
        "                                  style_max,\n",
        "                                  num_images,\n",
        "                                  batch_size,\n",
        "                                  image_size = (256, 256),\n",
        "                                  channels = 3,\n",
        "                                  s_shift_size = 1):\n",
        "    \n",
        "    h, w = image_size\n",
        "    counterfactual_images = []\n",
        "    indices_and_signs = s_indices_and_signs[:k]\n",
        "    print(\"Lenth\", len(indices_and_signs))\n",
        "    \n",
        "    for latent in tqdm.tqdm(latents):\n",
        "\n",
        "        latent = np.expand_dims(latent, axis=0)\n",
        "\n",
        "        expanded_latent = tf.tile(tf.expand_dims(latent, 1), [1, num_layers, 1])\n",
        "        base_prob = get_classifier_results(generator, expanded_latent)\n",
        "\n",
        "        if np.argmax(base_prob) == 1:\n",
        "            flip = False\n",
        "        elif np.argmax(base_prob) == 0:\n",
        "            flip = True\n",
        "\n",
        "        classifier_results = []\n",
        "        one_hots = []\n",
        "\n",
        "        for direction, sindex in indices_and_signs:\n",
        "\n",
        "            layer_idx, weight_idx = sindex_to_layer_idx_and_index(generator, sindex)\n",
        "\n",
        "            layer = generator.style_vector_calculator.style_dense_blocks[layer_idx]\n",
        "            layer_size = layer.dense_bias.weights[0].shape[1]\n",
        "\n",
        "            s_vals = tf.concat(generator.style_vector_calculator(latent, training=False)[0], axis=1).numpy()[0]\n",
        "\n",
        "            if (direction == 0 and flip == False) or (direction == 1 and flip == True):\n",
        "                shift = (style_min[sindex] - s_vals[sindex]) * s_shift_size\n",
        "\n",
        "            elif (direction == 1 and flip == False) or (direction == 0 and flip == True):\n",
        "                shift = (style_max[sindex] - s_vals[sindex]) * s_shift_size\n",
        "\n",
        "            one_hot_shift = shift * tf.expand_dims(tf.one_hot(weight_idx, layer_size), axis=0)\n",
        "            one_hots.append(one_hot_shift)\n",
        "            layer.dense_bias.weights[0].assign_add(one_hot_shift)\n",
        "        \n",
        "        counterfactual_image = call_synthesis(generator, expanded_latent, num_layers=num_layers)\n",
        "        counterfactual_images.append(tf.transpose(counterfactual_image, perm=[0, 2, 3, 1]).cpu())\n",
        "\n",
        "        for x, (_, sindex) in enumerate(indices_and_signs):\n",
        "            layer_idx, weight_idx = sindex_to_layer_idx_and_index(generator, sindex)\n",
        "            layer = generator.style_vector_calculator.style_dense_blocks[layer_idx]\n",
        "            layer.dense_bias.weights[0].assign_add(-one_hots[x])\n",
        "\n",
        "    counterfactual_dataset = tf.stack(counterfactual_images).cpu()\n",
        "    counterfactual_dataset = tf.reshape(counterfactual_dataset, [num_images // batch_size, batch_size, h, w, channels]).cpu()\n",
        "    \n",
        "    return counterfactual_dataset"
      ],
      "metadata": {
        "id": "NhUPHWQH0mdb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_fid(model, dataset, generated_dataset, size, normalize):\n",
        "    \n",
        "    save_original_mu = []\n",
        "    save_original_sigma = []\n",
        "    save_generated_mu = []\n",
        "    save_generated_sigma = []\n",
        "    \n",
        "    for i, batch in enumerate(tqdm.tqdm(dataset)):\n",
        "\n",
        "        dataset_batch = preprocess_inception(batch, size, False, normalize)\n",
        "        generated_dataset_batch = preprocess_inception(generated_dataset[i], size, True, normalize)\n",
        "        \n",
        "        original_act = model.predict(dataset_batch)\n",
        "        generated_act = model.predict(generated_dataset_batch)\n",
        "\n",
        "        original_mu, original_sigma = original_act.mean(axis=0), cov(original_act, rowvar=False)\n",
        "        generated_mu, generated_sigma = generated_act.mean(axis=0), cov(generated_act, rowvar=False)\n",
        "        \n",
        "        save_original_mu.append(original_mu)\n",
        "        save_original_sigma.append(original_sigma)\n",
        "        save_generated_mu.append(generated_mu)\n",
        "        save_generated_sigma.append(generated_sigma)\n",
        "        \n",
        "        del dataset_batch\n",
        "        del generated_dataset_batch\n",
        "    \n",
        "    original_mu = np.mean(save_original_mu, axis=0)\n",
        "    original_sigma = np.mean(save_original_sigma, axis=0)\n",
        "    generated_mu = np.mean(save_generated_mu, axis=0)\n",
        "    generated_sigma = np.mean(save_generated_sigma, axis=0)\n",
        "    ssdiff = np.sum((original_mu - generated_mu)**2.0)\n",
        "    covmean = sqrtm(original_sigma.dot(generated_sigma))\n",
        "\n",
        "    if iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = ssdiff + trace(original_sigma + generated_sigma - 2.0 * covmean)\n",
        "\n",
        "    return fid"
      ],
      "metadata": {
        "id": "-Hw1ogy70mfk"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid_images = \"./fid_images\"\n",
        "original_dir = os.path.join(fid_images, \"original_images\")\n",
        "generated_dir = os.path.join(fid_images, \"generated_images\")\n",
        "counterfactual_dir = os.path.join(fid_images, \"counterfactual_images\")\n",
        "fids_results_filepath = \"./fid_results.csv\"\n",
        "\n",
        "if not os.path.exists(fid_images):\n",
        "    os.mkdir(fid_images)\n",
        "    os.mkdir(original_dir)\n",
        "    os.mkdir(generated_dir)\n",
        "    os.mkdir(counterfactual_dir)"
      ],
      "metadata": {
        "id": "R0tEAg6Z3uIX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_tensor_image(folder, dataset):\n",
        "    for index, image in enumerate(dataset):\n",
        "        image = img_to_array(image[0])\n",
        "        save_img(os.path.join(folder, 'image' + str(index) + '.png'), image)"
      ],
      "metadata": {
        "id": "Agi7DfSA36LZ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fid_topk(original_dir,\n",
        "             generated_dir,\n",
        "             counterfactual_dir,\n",
        "             k,\n",
        "             latents,\n",
        "             generator,\n",
        "             classifier,\n",
        "             inception,\n",
        "             s_indices_and_signs,\n",
        "             style_min,\n",
        "             style_max,\n",
        "             cuda_rank=0):\n",
        "    \n",
        "    fids = []\n",
        "\n",
        "    fid_original_generated = calculate_fid_given_paths([original_dir, generated_dir], batch_size=50, device=cuda_rank, dims=2048)\n",
        "    fids.append(fid_original_generated)\n",
        "    \n",
        "    for i in range(k):\n",
        "        counterfactual_dataset = create_counterfactual_dataset(i+1, latents, generator,\n",
        "                                                       classifier, s_indices_and_signs,\n",
        "                                                       style_min, style_max,\n",
        "                                                       num_images, batch_size=batch_size)\n",
        "        save_tensor_image(counterfactual_dir, counterfactual_dataset)\n",
        "        fid_counterfactual = calculate_fid_given_paths([original_dir, counterfactual_dir], batch_size=50, device=cuda_rank, dims=2048)\n",
        "        fids.append(fid_counterfactual)\n",
        "    \n",
        "    return fids"
      ],
      "metadata": {
        "id": "gcvyB75q6EJQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_fids(fids, fids_results_filepath):\n",
        "    num_fids = len(fids)\n",
        "    column_names = [\"Original-Generated\"] + [\"Original-Attribute 1\"] + [\"Original-Attributes 1-\" + str(i) for i in range(2, num_fids)]\n",
        "    data = {column_names[i]:[format(fids[i], '.2f')] for i in range(num_fids)}\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(fids_results_filepath) "
      ],
      "metadata": {
        "id": "NNmx_P836Ntz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "num_images = latents.shape[0]\n",
        "image_size = (256, 256)\n",
        "channels = 3\n",
        "s_shift_size = 1\n",
        "\n",
        "original_dataset = tf.keras.utils.image_dataset_from_directory(\"/content/tensorflow-inception_images/Images\",\n",
        "                                                               labels=None,\n",
        "                                                               label_mode=None,\n",
        "                                                               class_names=None,\n",
        "                                                               image_size=(256, 256),\n",
        "                                                               batch_size=batch_size)\n",
        "save_tensor_image(original_dir, original_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INynshxv0uDM",
        "outputId": "b5b9afd1-dc09-487f-e93b-f21fdf02d300"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 250 files belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_dataset = decode_latents(latents, generator, num_images, batch_size)\n",
        "save_tensor_image(generated_dir, generated_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC57jfOA0uwG",
        "outputId": "43aa4d18-0920-40ff-e1b4-3c7fc5d57f01"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/250 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-21-44a4590ebe5a>:11: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.identity instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 250/250 [00:20<00:00, 12.16it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_attributes = 10\n",
        "fids = fid_topk(original_dir,\n",
        "                generated_dir,\n",
        "                counterfactual_dir,\n",
        "                num_attributes,\n",
        "                latents, generator,\n",
        "                classifier, inception,\n",
        "                s_indices_and_signs,\n",
        "                style_min, style_max)\n",
        "save_fids(fids, fids_results_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277,
          "referenced_widgets": [
            "4dcd82b5eb5a4037a54dbb41c67967ad",
            "c56c6a853cec4e15ab08ad8fe511aa5a",
            "d1499b6a6a774ead962aa6c713600746",
            "9a04181950a04055a7ded9a5f5139942",
            "8eac00d251ff4a19b611de2dcccf7ed9",
            "98439b1b8fb74018b88a18c643053397",
            "2dfc2645a8d04ed39a05f83176aa8658",
            "e5c5d0393de544879c3c9368128a9162",
            "7612606090a1407b8dcf0eadd9628a10",
            "d9ebc5b7dda5415dbe2f4ccfcc1316fb",
            "15faee2ce7cb42019e230e557a7368b4"
          ]
        },
        "id": "CKpkxBzz0xtf",
        "outputId": "1ce02d5d-c1d0-40d6-af50-789be6916d5a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4dcd82b5eb5a4037a54dbb41c67967ad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/91.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  2.60it/s]\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78.5034961551699\n",
            "Lenth 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 250/250 [00:23<00:00, 10.54it/s]\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.00it/s]\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lenth 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 250/250 [00:28<00:00,  8.79it/s]\n",
            "100%|██████████| 5/5 [00:01<00:00,  2.99it/s]\n",
            "100%|██████████| 5/5 [00:01<00:00,  3.07it/s]\n"
          ]
        }
      ]
    }
  ]
}