{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1bc7b002-0ae2-4593-b3d8-7edf36479282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(ckpt=None):\n",
    "    config = OmegaConf.load(\"models/ldm/att_ffhq/config_v4.yaml\")\n",
    "    if ckpt:\n",
    "        model = load_model_from_config(config, ckpt)\n",
    "    else:\n",
    "        model = instantiate_from_config(config.model)\n",
    "    return model\n",
    "\n",
    "\n",
    "class expand_greyscale(object):\n",
    "    def __init__(self, transparent):\n",
    "        self.transparent = transparent\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        channels = tensor.shape[0]\n",
    "        num_target_channels = 4 if self.transparent else 3\n",
    "\n",
    "        if channels == num_target_channels:\n",
    "            return tensor\n",
    "\n",
    "        alpha = None\n",
    "        if channels == 1:\n",
    "            color = tensor.expand(3, -1, -1)\n",
    "        elif channels == 2:\n",
    "            color = tensor[:1].expand(3, -1, -1)\n",
    "            alpha = tensor[1:]\n",
    "        else:\n",
    "            raise Exception(f'image with invalid number of channels given {channels}')\n",
    "\n",
    "        if not exists(alpha) and self.transparent:\n",
    "            alpha = torch.ones(1, *tensor.shape[1:], device=tensor.device)\n",
    "\n",
    "        return color if not self.transparent else torch.cat((color, alpha))\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, folder, image_size, transparent=False, aug_prob=0.):\n",
    "        super().__init__()\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for ext in EXTS for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "        assert len(self.paths) > 0, f'No images were found in {folder} for training'\n",
    "\n",
    "        convert_image_fn = convert_transparent_to_rgb if not transparent else convert_rgb_to_transparent\n",
    "        num_channels = 3 if not transparent else 4\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Lambda(convert_image_fn),\n",
    "            transforms.Lambda(partial(resize_to_minimum_size, image_size)),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(expand_greyscale(transparent))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None    \n",
    "    \n",
    "def default(value, d):\n",
    "    return value if exists(value) else d\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for i in iterable:\n",
    "            yield i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5213bec7-9cf0-4dbb-88a7-36eb0c60e519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "EXTS = ['jpg', 'jpeg', 'png']\n",
    "\n",
    "def convert_transparent_to_rgb(image):\n",
    "    if image.mode != 'RGB':\n",
    "        return image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "def resize_to_minimum_size(min_size, image):\n",
    "    if max(*image.size) < min_size:\n",
    "        return torchvision.transforms.functional.resize(image, min_size)\n",
    "    return image\n",
    "\n",
    "def set_data_src(folder='./', dataset_name=None, image_size=64, batch_size=1, num_workers=4, is_ddp=False, rank=0, world_size=1):\n",
    "    if dataset_name is None:\n",
    "        dataset = Dataset(folder, image_size)\n",
    "        num_workers = default(num_workers, NUM_CORES if not is_ddp else 0)\n",
    "\n",
    "        dataloader = DataLoader(dataset, num_workers=num_workers,\n",
    "                                     batch_size=math.ceil(batch_size / world_size),\n",
    "                                     shuffle=False, drop_last=True, pin_memory=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"This dataset is not supported yet. Please use dataset_name = None.\")\n",
    "\n",
    "    loader = cycle(dataloader)\n",
    "    return dataset, loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f49679fc-ec8d-4719-af73-aa09864bbde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./last.ckpt\n",
      "LatentDiffusion: Running in eps-prediction mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionWrapper has 400.92 M params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/aim/storage/structured/sql_engine/models.py:16: MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "model = get_model(\"./last.ckpt\")\n",
    "model = model.to(\"cuda\")\n",
    "sampler = DDIMSampler(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "71c7607d-6486-47da-8928-d423ea49b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 8\n",
    "ddim_steps = 20 # 200 or ddpm sample for better quality\n",
    "ddim_eta = 0.0\n",
    "scale = 3.0  # for unconditional guidance\n",
    "\n",
    "data_path = \"/root/Explaining-In-Style-Reproducibility-Study/data/Kaggle_FFHQ_Resized_256px/flickrfaceshq-dataset-nvidia-resized-256px/resized\"\n",
    "dataset, dataloader = set_data_src(folder=data_path, dataset_name=None, image_size=64, batch_size=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fdab8d-6040-49ef-af79-12e3e14c175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "test_dim = 251  # a dim between [0-511]\n",
    "sign = -1 # [1, -1]\n",
    "# TODO: try with unconditional_guidance_scale\n",
    "# TODO: ???batch size of cond and batch size of sample\n",
    "with torch.no_grad():\n",
    "    batch = next(data_iter).cuda()\n",
    "    with model.ema_scope():\n",
    "        uc = model.get_learned_conditioning(\n",
    "            {\"class_label\": model.classifier.classify_images(batch),\n",
    "             \"image_batch\": batch}\n",
    "        )\n",
    "        logits = model.classifier.classify_images(batch)\n",
    "        c = model.get_learned_conditioning(\n",
    "            {\"class_label\": model.classifier.classify_images(batch),\n",
    "             \"image_batch\": batch}\n",
    "        )\n",
    "        print(torch.softmax(logits, dim=1)[:, 0])\n",
    "        \n",
    "#         samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "#                                          conditioning=c,\n",
    "#                                          batch_size=8,\n",
    "#                                          shape=[3, 64, 64],\n",
    "#                                          verbose=False,\n",
    "#                                          eta=ddim_eta)\n",
    "        samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                             conditioning=c,\n",
    "                                             batch_size=8,\n",
    "                                             shape=[3, 64, 64],\n",
    "                                             verbose=False,\n",
    "                                             unconditional_guidance_scale=scale,\n",
    "                                             unconditional_conditioning=uc, \n",
    "                                             eta=ddim_eta)\n",
    "        # samples_ddim, _ = model.sample(cond=c, batch_size=8, return_intermediates=True)\n",
    "        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "\n",
    "        x_samples_ddim = torch.clamp(x_samples_ddim, min=0.0, max=1.0)\n",
    "        logits = model.classifier.classify_images(x_samples_ddim)\n",
    "        print(torch.softmax(logits, dim=1)[:, 0])\n",
    "    \n",
    "        dirs = torch.Tensor([0.0002]*8).cuda() * sign\n",
    "        c[:, 0, test_dim] += dirs\n",
    "        samples_ddim2, _ = sampler.sample(S=ddim_steps,\n",
    "                                             conditioning=c,\n",
    "                                             batch_size=8,\n",
    "                                             shape=[3, 64, 64],\n",
    "                                             verbose=False,\n",
    "                                             unconditional_guidance_scale=scale,\n",
    "                                             unconditional_conditioning=uc, \n",
    "                                             eta=ddim_eta)\n",
    "        x_samples_ddim2 = model.decode_first_stage(samples_ddim2)\n",
    "        x_samples_ddim2 = torch.clamp(x_samples_ddim2, min=0.0, max=1.0)\n",
    "        \n",
    "        c[:, 0, test_dim] += dirs\n",
    "        samples_ddim3, _ = sampler.sample(S=ddim_steps,\n",
    "                                             conditioning=c,\n",
    "                                             batch_size=8,\n",
    "                                             shape=[3, 64, 64],\n",
    "                                             verbose=False,\n",
    "                                             unconditional_guidance_scale=scale,\n",
    "                                             unconditional_conditioning=uc, \n",
    "                                             eta=ddim_eta)\n",
    "        x_samples_ddim3 = model.decode_first_stage(samples_ddim3)\n",
    "        x_samples_ddim3 = torch.clamp(x_samples_ddim3, min=0.0, max=1.0)\n",
    "        \n",
    "        \n",
    "grid = torch.stack([batch, x_samples_ddim, x_samples_ddim2, x_samples_ddim3], 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=8)\n",
    "        \n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a7ede2d-c0a7-4812-93e3-198b4f241fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 512])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.0001\n",
    "combos = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch = next(data_iter).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ccb8723a-c633-4128-9cf3-4e2c297a353c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6584e-04, 9.9973e-01],\n",
       "        [9.9916e-01, 8.4361e-04],\n",
       "        [1.3462e-01, 8.6538e-01],\n",
       "        [9.6271e-01, 3.7286e-02],\n",
       "        [7.6450e-01, 2.3550e-01],\n",
       "        [9.9598e-01, 4.0245e-03],\n",
       "        [9.9307e-01, 6.9310e-03],\n",
       "        [9.9824e-01, 1.7632e-03]], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4e620eb-b354-45ec-98c2-17ae3529b764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0027, device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46ec55cf-76e9-4f28-a12d-70dabdf23c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "classifier = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eeff3d-1a72-446a-a127-11305cfd392f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
